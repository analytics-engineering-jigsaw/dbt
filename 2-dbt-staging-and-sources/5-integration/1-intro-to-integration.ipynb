{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70e74bad-c1bd-474b-a0c0-0e0b302e2bf4",
   "metadata": {},
   "source": [
    "# Introduction to Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36378ebd-2eb6-4f8c-912a-c0384fd548a4",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce6fcf8-dee2-4feb-bcea-9fb638c6a123",
   "metadata": {},
   "source": [
    "So far we have seen how to use DBT to access data from our data warehouse, and then perform some initial cleanup on that data through staging.  In this lesson, we'll move onto the step after staging, which is integration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1a291f-a0c4-49fa-b7a1-14d81c7fbc4f",
   "metadata": {},
   "source": [
    "### Combining Information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3c82ba-eab5-4f90-9fde-875815a7d704",
   "metadata": {},
   "source": [
    "Before moving to the integration layer, let's just review our general pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6122d938-1713-4f32-9a57-794775e439bb",
   "metadata": {},
   "source": [
    "<img src=\"./dbt-pipeline.jpg\" width=\"70%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8a6555-8d7e-4f24-b850-8b7bab9045e1",
   "metadata": {},
   "source": [
    "So far, we have defined our sources with the `sources.yaml` file, and then performed some initial transformations of data in staging.  In the staging layer, our data is still segmented out by source, but in the integration layer, we'll combine information from various sources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b482444-b834-44b8-807e-a9f24884be2f",
   "metadata": {},
   "source": [
    "This point is re-emphasized if we look at the file structure of our current project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b89a0da-f3ed-485b-8aeb-f1e79636ecb1",
   "metadata": {},
   "source": [
    "> As we can see below, we still have our hubspot folder separated from our rds folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b186cb-c68b-42a3-9a8c-d8938e20fe2f",
   "metadata": {},
   "source": [
    "> <img src=\"./staging-models.png\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afd6871-f573-4a5e-9d13-48873a95be6e",
   "metadata": {},
   "source": [
    "So in the future, we will create a folder under models called `integration`, and begin to combine our data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e4e6e4-cfeb-420b-a26d-1ead845e1658",
   "metadata": {},
   "source": [
    "### Our plan going forward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8018aa72-b553-4e70-a507-6a6c59cc27e6",
   "metadata": {},
   "source": [
    "Here's how we'll begin to combine our data.  \n",
    "\n",
    "1. We'll combine our data by selecting from each source and stacking our data on top of one another with a `union all` function.  \n",
    "\n",
    "> Remember, that [the union function](https://docs.snowflake.com/en/sql-reference/operators-query.html#union-all) combines multiple select statements.  And the `union all` function includes any duplicates.\n",
    "\n",
    "2. Now that our data is stacked on top of each other, we'll then deduplicate our data by performing a group by on a column whose information should not be duplicated -- for example, a company name, for companies.  \n",
    "\n",
    "3. Our combined data will have different primary keys than our source data, so we'll then need to re-associate our models by use of primary keys and foreign keys.\n",
    "\n",
    "We'll move through these steps in the following lessons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846254a4-79c6-4bd7-bafb-0b5401a80b4e",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b682cf30-f532-40e2-81d2-3ffec32bd2e6",
   "metadata": {},
   "source": [
    "In this lesson, we re-introduced our integration layer.  So far we have kept our data segmented by it's various sources.  In the integration layer, we'll combine the records in our sources together.  And we'll do so by:\n",
    "\n",
    "1. Combining our data -- including duplicates -- with the `union all` function. \n",
    "2. Deduplicating our data by grouping by on a unique attribute across the rows of data\n",
    "3. Properly associating data to maintain our model relations now that our data has different ids."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39958bb-b4f0-4f99-ba8b-e80e896d0d7d",
   "metadata": {},
   "source": [
    "### Sources\n",
    "\n",
    "[Rittman Analytics Data Centralization](https://rittmananalytics.com/blog/2020/5/28/introducing-the-ra-warehouse-dbt-framework-how-rittman-analytics-does-data-centralization)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
